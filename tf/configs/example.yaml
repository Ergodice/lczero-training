%YAML 1.2
---
name: 'reluintermediatetalking'
gpu: 0
dataset:
  num_chunks: 5_000_000
  allow_less_chunks: true
  train_ratio: 0.95
  sort_type: name
  input: 'C:/Users/micro/leeladata/train/*/'
  #input: 'C:/Users/micro/leeladata/train/training-run2-test77-20211201-0018/'
  experimental_v5_only_dataset: false
  train_workers: 8
  test_workers: 4
training:
    precision: half
    swa: true
    swa_output: true    
    swa_max_n: 10
    swa_steps: 100
    max_grad_norm: 1.0
    batch_size: 1024
    num_batch_splits: 2
    q_ratio: 0
    value_focus_min: 1.0
    value_focus_slope: 0.0
    lookahead_optimizer: false
    renorm: true
    renorm_max_r: 1.0
    renorm_max_d: 0.0
    test_steps: 2500
    # validation_steps: 5000
    num_test_positions: 131_072
    train_avg_report_steps: 2500
    total_steps: 500_000
    checkpoint_steps: 10_000 
    
    shuffle_size: 250_000
    warmup_steps: 4000
    mask_legal_moves: true
    lr_values:
        - 0.2
        - 0.16
        - 0.128
        - 0.1024
        - 0.08192  # 500k
        - 0.06554
        - 0.05243
        - 0.04194
        - 0.03355
        - 0.02684  # 1M
        - 0.02147
        - 0.01718
        - 0.01374
        - 0.01100
        - 0.00880  # 1.5M
        - 0.00704
        - 0.00563
        - 0.00450
        - 0.00360
        - 0.00288  # 2M
    lr_boundaries:
        - 100_000 
        - 200_000
        - 300_000
        - 400_000
        - 500_000
        - 600_000
        - 700_000
        - 800_000
        - 900_000
        - 1_000_000
        - 1_100_000
        - 1_200_000
        - 1_300_000
        - 1_400_000
        - 1_500_000
        - 1_600_000
        - 1_700_000
        - 1_800_000
        - 1_900_000
    policy_loss_weight: 1.0
    value_loss_weight: 1.0
    reg_term_weight: 1.0
    moves_left_loss_weight: 1.0
    path: 'C:/Users/micro/Documents/GitHub/lczero-attention/networks'
model:
    filters: 128                         # Number of filters
    residual_blocks: 0                   # Number of blocks
    se_ratio: 8   # Squeeze Excite structural network architecture.                 
    embedding_size: 128
    policy_embedding_size: 128
    value_embedding_size: 32
    moves_left_embedding_size: 8
    encoder_layers: 5                   # number of intermediate attention layers in the policy head
    encoder_heads: 8                     # number of attention heads in encoder layers
    #kq_heads: 8
    #inner_heads: 8
    #v_heads: 8
    qk_d_model: 128                 # size of the Q, K, & V vectors in encoder layers -- divisible by encoder_heads
    v_d_model: 128
    encoder_dff: 512                    # size of the largest dense layer in encoder layer ffn
    policy_d_model: 128                  # size of the query and key vectors in final attention layer
    dropout_rate: 0.0                    # the dropout rate used for weight regularization of attention during training
    policy: 'attention'                  # new option: attention
    value: 'wdl'
    moves_left: 'v1'
    input_type: "canonical_100"



    # "It's big brain time" -- Markiplier
    use_fullgen: false # Always recommended
    fullgen_hidden_channels: 4
    fullgen_hidden_sz: 128
    fullgen_out_maps: 4
    fullgen_history: 1
    fullgen_relu: false
    use_fullgen_scaling: false
    use_antifullgen: false

    # There's no way this works
    use_policy_fullgen: false # Not recommended
    policy_fullgen_hidden_channels: 64
    policy_fullgen_hidden_sz: 1024

    # Monroe's magical menagerie
    talking_heads: true # Always recommended
    logit_gate: true # Always recommended
    weight_gen: false # Not recommended
    attention_transpose: false # Not recommended
    gating_everywhere: false
    split_talking_heads: false

    # Daniel's dynamic designs
    dydense_temp_start: 30 # should probably move to training
    dydense_temp_anneal_steps: 100_000 #!!! slower?
    dydense_usage: '' # 'qkvo12' for query, key, value, out, dff1, dff2; must be string
    dydense_kernels: 2 # if dydense use
    dydense_pc: false # channelwise: true or false
    use_dyrelu: false # whether to use dyrelu at dff
    dytalking_heads: false # !!! not implemented yet

    # Buckets
    buckets: 1